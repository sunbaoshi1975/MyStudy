license()
demo()
colors()
local({pkg <- select.list(sort(.packages(all.available = TRUE)),graphics=TRUE)
if(nchar(pkg)) library(pkg, character.only=TRUE)})
setRepositories()
utils:::menuInstallPkgs()
a <- available.packages()
head(rownames(a), 10)
helps()
help()
help("slidify")
install.packages("slidify")
q()
install.pachages("KernSmooth")
install.packages("KernSmooth")
help(libray)
help("libray")
??load
?load
library(KernSmooth)
a <- available.packages()
print a
a
a <- available.packages()
head(rownames(a), 3)
head(rownames(a), 10)
library(devtools)
q()
a <- available.packages()
head(rownames(a), 10)
q()
install.packages("caret")
q()
install.packages("iris")
library(iris)
install.packages("iris")
data(iris)
library(ggplot2)
names(iris)
dimnames
colnames
colnames(iris)
names(iris)
dimnames(iris)
dimnames(iris, 1)
dimnames(iris, 2)
help(dimnames)
table(iris$Species)
library(carrot)
library(carret)
library(caret)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FLASE)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(traning); dim(testing)
dim(training); dim(testing)
qplot(Petal.Width, Sepal.Width, color=Species, data=training)
qplot(Petal.Width, Sepal.Width, colour=Species, data=training)
modFit <- train(Species ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree"
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattel)
install.packages("rattel")
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
help(fancyRpartPlot)
modFit$finalModel
fancyRpartPlot(modFit$finalModel)
help(plot)
rattle()
fancyRpartPlot(modFit$finalModel)
predict(modFit, newdata=testing)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(ozone, package="ElemStatLearn")
head(ozone)
ss <- sample(1:dim(ozone)[1], replace=T)
head(ss)
ll <- matrix(NA, nrow=10, ncol=155)
for(i in 1:10) {
ss <- sample(1:dim(ozone)[1], replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone, data=ozone0, span=0.2)
ll[i,] <- predict(loess0, newdata.frame(ozone=1:155))
}
ll <- matrix(NA, nrow=10, ncol=155)
for(i in 1:10) {
ss <- sample(1:dim(ozone)[1], replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone, data=ozone0, span=0.2)
ll[i,] <- predict(loess0, newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
for(i in 1:10) {lines(1:155, ll[i,], col="grey", lwd=2)}
lines(1:155, apply(11,2,mean), col="red", lwd=2)
plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
for(i in 1:10) {lines(1:155, ll[i,], col="grey", lwd=2)}
lines(1:155, apply(ll,2,mean), col="red", lwd=2)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B=10, bagControl=bagControl(fit=ctreeBag$fit,
predict=ctreeBag$pred,
aggregate=ctreeBag$aggregate))
plot(ozone$ozone, ozone$temperature, col='lightgrey', pch=19)
plot(ozone$ozone, predict(treebag$fit[[1]]$fit,predictors), col='red', pch=19)
plot(ozone$ozone, predict(treebag,predictors), col='blue', pch=19)
treebag <- bag(predictors, temperature, B=10,
bagControl=bagControl(fit=ctreeBag$fit,
predict=ctreeBag$pred,
aggregate=ctreeBag$aggregate))
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(traning); dim(testing)
data(iris)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(traning); dim(testing)
dim(training); dim(testing)
modFit <- train(Species ~ ., data=training, method="rf", prox=TRUE)
modFit <- train(Species ~ ., data=training, method="rf", prox=TRUE)
modFit
getTree(modFit$finalModel, k=2)
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species, data=training)
p
p + geom_point(aes(x=Petal.Width, y=Petal.Length, col=Species), size=5, shape=4, data=irisP)
help(aes)
pred <- predict(modFit, testing); testing$predRight <- pred==testing$Species
table(pred, testing$Species)
gplot(Petal.Width, Petal.Length, col=predRight, data=testing, main="newdata Preditions")
qplot(Petal.Width, Petal.Length, col=predRight, data=testing, main="newdata Preditions")
clear
clr
library(ISLR)
install.packages("ISLR")
library(ISLR)
data(Wage)
name(Wage)
names(Wage)
Wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
print(modFit)
modFit <- train(Wage ~ ., method="gbm", data=training, verbose=FALSE)
print(modFit)
qplot(predict(modFit, testing), wage, data=testing)
data(iris)
library(ggplot2)
library(caret)
names(iris)
table(iris$Species)
modlda = train(Species ~ ., data=training, method="lda")
modnb = train(Species ~ ., data=training, method="nb")
plda = predict(modlda, testing); pnb = predict(modnb, testing)
table(plda, pnb)
modlda = train(Species ~ ., data=training, method="lda")
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
modlda = train(Species ~ ., data=training, method="lda")
modnb = train(Species ~ ., data=training, method="nb")
modlda = train(Species ~ ., data=training, method="lda")
modnb = train(Species ~ ., data=training, method="nb")
plda = predict(modlda, testing); pnb = predict(modnb, testing)
table(plda, pnb)
equalPredictions = (plda == pnb)
qplot(Petal.Width, Sepal.Width, color=equalPredictions, data=testing)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
dim(training); dim(testing)
modFit <- train(Case ~ ., method="rpart", data=training)
seed(125)
seeds(125)
set.seed(125)
modFit <- train(Case ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Case ~ ., method="rpart", data=training)
print(modFit$finalModel)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Case ~ ., method="rpart", data=training)
print(modFit$finalModel)
library(AppliedPredictiveModeling)
library(caret)
library(rattle)
rm(list = ls())
data(segmentationOriginal)
names(segmentationOriginal)
training = subset(segmentationOriginal, Case == "Train")
testing = subset(segmentationOriginal, Case == "Test")
dim(training); dim(testing)
set.seed(125)
modFit = train(Case ~ ., method="rpart", data=training)
print(modFit$finalModel)
set.seed(125)
model = train(Case ~ ., method="rpart", data=training)
print(model)
set.seed(125)
model = train(Case ~ ., method="rpart", data=training)
model = train(Case ~ ., method='rpart', data=training)
set.seed(125)
modFit <- train(Case ~ ., method="rpart", data=training)
training = subset(segmentationOriginal, Case == "Train")
testing = subset(segmentationOriginal, Case == "Test")
dim(training); dim(testing)
set.seed(125)
modFit <- tra
setwd("D:/4 - Works/GitHub/MyStudy/PML Course Project")
rawData_training <- read.csv('pml-training.csv', header=TRUE, sep=',', na.strings=c("NA","#DIV/0!",""))
rawData_testing <- read.csv('pml-testing.csv', header=TRUE, sep=',', na.strings=c("NA","#DIV/0!",""))
training <- rawData_training[, -c(1:7)]
testing <- rawData_testing[, -c(1:7)]
features = dim(training)[2]
suppressWarnings(training[,-c(features)] <- sapply(training[,-c(features)], as.numeric))
suppressWarnings(testing[,-c(features)] <- sapply(testing[,-c(features)], as.numeric))
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
dim(training);dim(testing)
fitCtrl <- trainControl(method = "cv", number = 5)
modFit <- train(classe ~ ., method="rpart", data=training, na.action = na.pass, trControl = fitCtrl)
fitCtrl <- trainControl(method = "cv", number = 5)
library(ggplot2)
library(caret)
library(randomForest)
fitCtrl <- trainControl(method = "cv", number = 5)
modFit <- train(classe ~ ., method="rpart", data=training, na.action = na.pass, trControl = fitCtrl)
vi = varImp(modFit)
impfeatures <- rownames(vi$importance)[vi$importance > 0]
impfeatures
set.seed(201502)
modFeatures = c(impfeatures, "classe")
training <- training[modFeatures]
inTrain <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
selftraining <- training[inTrain,]
selftesting <- training[-inTrain,]
dim(selftraining, selftesting)
dim(selftraining); dim(selftesting)
modrf = randomForest(classe ~ ., data=selftraining, na.action=na.omit)
modrf
modnb = train(classe ~ ., data=selftraining, method="nb")
modnb
table(predrf, prednb)
predrf = predict(modrf, selftesting)
prednb = predict(modnb, selftesting)
predrf; prednb
table(predrf, prednb)
confusionMatrix(predrf, selftesting$classe)
##### nb: Sample Accuracy
confusionMatrix(prednb, selftesting$classe)
predrf = predict(modrf, testing)
finalpred = predict(modrf, testing)
finalpred
testing
varImp(mdlrf)
varImp(modrf)
finalvi = varImp(mdlrf)
finalvi = data.frame(var = 1:nrow(finalvi), imp = finalvi$Overall)
finalvi[order(finalvi$imp, decreasing = T),]
finalvi = varImp(modrf)
finalvi = data.frame(var = 1:nrow(finalvi), imp = finalvi$Overall)
finalvi[order(finalvi$imp, decreasing = T),]
finalvi
finalvi = varImp(modrf)
finalvi
order(vi,decreasing = T)
order(finalvi,decreasing = T)
finalvi
write.table(finalpred, file="results.txt", quote=TRUE, sep=",", col.names=FALSE, row.names=FALSE)
help(load.table)
help(read.table)
answers <- read.table('results.txt', header=FALSE, quote=TRUE, sep=",")
answers <- read.table('results.txt', header=FALSE, sep=",")
answers
pml_write_files(answers)
pml_write_files(answers)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
setwd("D:/4 - Works/GitHub/MyStudy/PML Course Project")
answers <- read.table('results.txt', header=FALSE, sep=",")
answers
pml_write_files(answers)
length(answer)
length(answers)
answers = answers''
answers = answers'
size(x)
dim(answers)
finalpred
dim(finalpred)
dim(answers)[2]
dim(answers)[1]
t(answers)
pml_write_files(t(answers)
)
