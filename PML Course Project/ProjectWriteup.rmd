---
title: "Practical Machine Learning Course Project"
author: "Baoshi Sun"
output:
  html_document:
    keep_md: yes
---

## Introduction
The purpose of this project is to predict the manner or classe of people in which they do exercise. Accelerometer data of 6 participants were collected during they perform barbell in 5 different ways (either correctly or incorrectly). This research is based on these example data.    
More background information can be found at http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Preparation
### Setting up working environment
* Woking Directory
A directory named 'PML Course Project' is built as the main working directory. All files related to this project will be stored in here.
There is a sub-directory named 'submission' under the main directory. It is used to store the 20 prediction result files corresponding to the 20 different test cases.
Please refer to the [README.md]() for detailed information about directories and files under this project.

* Raw Data
The example data include both traning data and testing data, which were downloaded on Feb 5rd from the following URLs respectively:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
They are in CVS format. The main working directory also hold a copy of the two data files.

* Software Packages
To finish the project, a couple of software needs to be installed.   
-- R Version 3.1.2 64bit  
-- RStudio Version 0.98.1102  
-- notepad++ Version 6.7.4  
The following packages in R are expected to be used:
-- knitr - markdown tools  
-- caret  
-- randomForest  
-- ggplot2  

## Planning
### Raw Data Exploration
In order to have an overall idea about the given observations, raw data has to be imported and reviewed.

```{r}
# read original datasets, both training and testing examples
rawData_training <- read.csv('pml-training.csv', header=TRUE, sep=',', na.strings=c("NA","#DIV/0!",""))
rawData_testing <- read.csv('pml-testing.csv', header=TRUE, sep=',', na.strings=c("NA","#DIV/0!",""))

# Raw Data Exploration
dim(rawData_training); dim(rawData_testing)
names(rawData_training)
table(rawData_training$user_name)
table(rawData_training$classe)
head(rawData_training, 3)
```

We can see there are 19622 and 20 examples in the training and testing datasets respectively, while the original data set contains 159 features (excluding the first column 'X', which is believed to be the row index). We can also tell the response variable is called "classe" and variable "user_name" stands for the name of participants.   
> [1] 19622   160
> [1]  20 160
> 
> [1] "X"                        "user_name"                "raw_timestamp_part_1"  
> [4] "raw_timestamp_part_2"     "cvtd_timestamp"           "new_window"  
> [7] "num_window"               "roll_belt"                "pitch_belt"  
> ......  
> [157] "magnet_forearm_x"         "magnet_forearm_y"         "magnet_forearm_z"  
> [160] "classe"  

6 participants and 5 classes (from A to E) are identified, which are corresponding to the project description.
> adelmo carlitos  charles   eurico   jeremy   pedro  
> 3892     3112     3536     3070     3402     2610  

> A    B    C    D    E  
> 5580 3797 3422 3216 3607  

When We go a little deeper into some data samples, we noticed these facts:
* There are four types of sensors. They are "roll_belt", "pitch_belt", "yaw_belt" and "total_accel_belt".
* A high percentage of variables have "NA" values.
* Among the 159 variables, some of they may have no relevancy to this research.
* Some variables have values whose variance closes to zero. At this moment, we are not sure if they should be trimmed off.
Therefore, during implementation phase data preprocessing should be applied.

```{r}
# number of rows with no NA values
sum(complete.cases(rawData_training))
# number of columns with at least one NA value
sum(sapply(names(rawData_training), function(x) any(is.na(rawData_training[,x]))))
```
>  [1] 0  
>  [1] 100  

Furthermore, as we can see above, every example (row) contains at lease one NA value and there are 100 features (columns) with at lease one NA value. Therefore, it seems to be important to deal with those NA values properly. A couple of options will be discussed during data cleaning of implementation phase.

### Model Selection
The project is a classification question. According to what I learned from the course, RPart, Bagging, Random Forests, Boosting, LDA and Naive Bayes are all suitable. Considering a sub 20 thousand dataset, we don't have to worried about the speed. Random Forests is a outstanding choice. With rf, we can take the advantage for its accuracy. In addition, "In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error."[[Random Forests by Leo Breiman and Adele Cutler](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr)]    

As a comparison, Naive Bayes algorithm is also applied. Due to the time constrains, I've decided not to try others algorithms. However, sometime after the course I'd like to make a fully comparison among all of above algorithms regarding this project.

## Implementation
### Data cleaning
For data cleaning, We have 3 tasks to do:   
- 1. To delete irrelevant columns (the first 7) and convert non-number fields except "classe" column to number  
```{r}
## 1. Delete irrelevant columns (the first 7)
training <- rawData_training[, -c(1:7)]
testing <- rawData_testing[, -c(1:7)]
## Convert everything except "classe" (last column) to numbers
features = dim(training)[2]
suppressWarnings(training[,-c(features)] <- sapply(training[,-c(features)], as.numeric))
suppressWarnings(testing[,-c(features)] <- sapply(testing[,-c(features)], as.numeric))
```
- 2. To deal with columns with NA values
- 3. To trim off features with near zero variance [optional]  
Features with near zero variance may slow down the processing speed without improving the accuracy of the final model significantly. It should be a good practice to perform this step properly (by using function nearZeroVar()). However, given the limited amount of training data, we don't think the speed would be a major consideration. We would like to mark this task as optional one and skip it in this turn.  

It is obvious that the focus for data cleaning is to deal with NA values (task 2). We have 3 options in this case:   
- Option 1: to delete any columns containing NAs, which is the simplest method, but may lose information   
- Option 2: just remove columns with all or an excessive ratio of NAs. The threshold can be defined. For example, we choose 20% here.   
- Option 3: to convert NAs with numbers (e.g. 0), which is an easy way too, but will introduce additional assumptions   

Please refer to the following code lists:

```{r}
# Option 1: to delete any columns containing NAs, simplest method but may lose information
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
dim(training);dim(testing)
```
> [1] 19622    53  
> [1] 20 53  

As we can see the 100 features are removed from both datasets.

```{r}
### Option 2: just remove columns with all or an excessive ratio of NAs. The threshold can be defined.
#### We choose 80% here
Threshold_NARatio = 0.8
ExcessiveNAsCol <- (colSums(is.na(training)) > (nrow(training) * Threshold_NARatio))
training <- training[!ExcessiveNAsCol]
testing <- testing[!ExcessiveNAsCol]
dim(training);dim(testing)
```
> [1] 19622    53  
> [1] 20 53  

When the threshold is 80%, we still expel the same 100 features as option 1 does. As computed, as long as the threshold is under 98%, the result stays the same. Only if we set the threshold upto 98%, we can encase more features.   
> ## Threshold_NARatio = 0.98   
> [1] 19622   134  
> [1] 20 134  

```{r}
### Option 3: to convert NAs with numbers (e.g. 0), easy way but will introduce additional assumptions
NAtoNum = 0.0
training <- replace(training, is.na(training), NAtoNum)
testing <- replace(testing, is.na(testing), NAtoNum)
### check number of columns with at least one NA value
sum(sapply(names(training), function(x) any(is.na(training[,x]))))
```
> [1] 0  

After this operation, We have no NA value anymore.    

Based on above analysis, I decided to run two rounds of modeling for feature selection. The first round goes with option 1 (52 features). And by using option 2 for the second round, the model will have 133 features with Threshold_NARatio of 98%. Then I can make a comparison and decide how many features I should keep.

### Feature Selection
For feature selection, I tried to use rpart with 5-fold cross validation. As we talked above, I've run the test twice and compared the results of variable importance. Although the accuracy of rpart is unacceptable (less than 51%), I can still have the result of variable importance and decide the feature selection.

```{r}
library(caret)
library(randomForest)

fitCtrl <- trainControl(method = "cv", number = 5)   
modFit <- train(classe ~ ., method="rpart", data=training, 
  na.action = na.pass, trControl = fitCtrl)   
modFit
vi = varImp(modFit)
vi
```

#### Round 1 - 52 predictors (removed all columns with NA value)
#### Round 2 - 133 predictors (removed columns with ratio of NA value >= 98%)
>                        Overall  
> pitch_forearm           100.00  
> roll_forearm             72.97  
> roll_belt                70.36  
> magnet_dumbbell_y        49.51  
> accel_belt_z             43.02  
> magnet_belt_y            40.80  
> yaw_belt                 40.41  
> magnet_dumbbell_z        36.37  
> total_accel_belt         35.34  
> magnet_arm_x             27.09  
> accel_arm_x              26.33  
> roll_dumbbell            18.67  
> accel_dumbbell_y         15.60  
> roll_arm                 15.28  
> skewness_roll_dumbbell    0.00  
> stddev_roll_arm           0.00
> gyros_forearm_y           0.00
> skewness_roll_belt        0.00
> avg_roll_forearm          0.00
> accel_belt_x              0.00   

As can be seen we had almost the same variable importance for the two rounds. And I am going to take the 14 predictors whose importance is greater than 0 to perform to real model training.  

```{r}
# Weed out 0 importance features
impfeatures <- rownames(vi$importance)[vi$importance > 0]
```

### Creation of Working Datasets
Notes: to reproduce the same result set.seed(201502) should be called at every model training circle.    
Training dataset was splitted into sub-training (70%) and self-testing (30%) for cross validation
```{r}
set.seed(201502)
modFeatures = c(impfeatures, "classe")
training <- training[modFeatures]
inTrain <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
selftraining <- training[inTrain,]
selftesting <- training[-inTrain,]
```

### Model Training
#### rf & nb
Notes: In order to speed up compiling, I commented out the NB modeling code in Rmd file. Please feel free to open and run the code on your computer.
```{r}
# rf
modrf = randomForest(classe ~ ., data=selftraining, na.action=na.omit)
# nb
#modnb = train(classe ~ ., data=selftraining, method="nb")
```

By using randomForest package, rf model was generated quickly, whereas nb took a long time to output the model.
Then the two models were applied to self-testing dataset to see the accuracy of each algorithms.

#### Model Self-test
```{r}
# Model Self-test   
predrf = predict(modrf, selftesting)   
#prednb = predict(modnb, selftesting)   
```

Please refer to the confusion matrix for the random forests model.  
As can be seen, the accuracy is prefect good, and reaches 99.24%.
However, the accuracy of NB is not acceptable, only got 70.57%. 

#### Sample Accuracy
```{r}
##### rf: Sample Accuracy
confusionMatrix(predrf, selftesting$classe)
##### nb: Sample Accuracy
#confusionMatrix(prednb, selftesting$classe)
```

>           Reference  
> Prediction    A    B    C    D    E  
>          A 1670    1    0    0    0  
>          B    4 1130   10    0    1  
>          C    0    7 1013   10    1  
>          D    0    1    3  951    4  
>          E    0    0    0    3 1076  
> 
> Overall Statistics
>                                           
>                Accuracy : 0.9924          
>                  95% CI : (0.9898, 0.9944)  
>     No Information Rate : 0.2845          
>     P-Value [Acc > NIR] : < 2.2e-16       
>                                           
>                   Kappa : 0.9903          
>  Mcnemar's Test P-Value : NA              

### Prediction on Testing Dataset
Finally, let's use the achieved random forests model to predict the real testing dataset. 
Results can be seen below. And they are also stored into a text file.

```{r}
finalpred = predict(modrf, testing)   
finalpred   
write.table(finalpred, file="results.txt", quote=TRUE, sep=",", col.names=FALSE, row.names=FALSE)
```

Here comes the predicted results.
>  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20   
>  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B    

## Results
### Accuracy
With the randomForest package, we generated a high accurate model. The final model has 99.24% accuracy, 95% CI (0.9898, 0.9944) and above 0.99 Kappa.

### Out of Sample Error
With a confusion matrix, we can see the number of Out of sample error is quite low.  
Two operations we conducted may help control the out of sample error and avoid over fitting.
* Early weed out less relevancy features: from 153 to 14
* Cross validation by splitting training dataset into final training partition and self-testing partition.

### Variable Importance in the final model
As we can see, the variable importance in the final model is notably different from the list that we worked out with rpart method earlier.  
In the final model, 'roll_belt', 'yaw_belt' and 'agnet_dumbbell_z' are the top three important features; 
while in the rpart model 'pitch_forearm', 'roll_forearm' and 'roll_belt' occupy the first three positions.

```{r}
finalvi = varImp(modrf)
finalvi
```
  
## Conclusions
This is a great hand-on assignment. Although the final model is not complicated at all, the data exploration phase is quite a job. 
In the real world, we are encountering the shift from information scarcity to surfeit. 
To identify 'information' from 'noise' is one of the most important work for a data scientist.    
Another trick I learned from the project is the right package will definitely speed up my work. 
Caret is a versatile tool box, but it seems not as efficient as other specialty package. 
For example, randomForest package is extremely faster than 'rf' in caret.

