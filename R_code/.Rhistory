license()
demo()
colors()
local({pkg <- select.list(sort(.packages(all.available = TRUE)),graphics=TRUE)
if(nchar(pkg)) library(pkg, character.only=TRUE)})
setRepositories()
utils:::menuInstallPkgs()
a <- available.packages()
head(rownames(a), 10)
helps()
help()
help("slidify")
install.packages("slidify")
q()
install.pachages("KernSmooth")
install.packages("KernSmooth")
help(libray)
help("libray")
??load
?load
library(KernSmooth)
a <- available.packages()
print a
a
a <- available.packages()
head(rownames(a), 3)
head(rownames(a), 10)
library(devtools)
q()
a <- available.packages()
head(rownames(a), 10)
q()
install.packages("caret")
q()
install.packages("iris")
library(iris)
install.packages("iris")
data(iris)
library(ggplot2)
names(iris)
dimnames
colnames
colnames(iris)
names(iris)
dimnames(iris)
dimnames(iris, 1)
dimnames(iris, 2)
help(dimnames)
table(iris$Species)
library(carrot)
library(carret)
library(caret)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FLASE)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(traning); dim(testing)
dim(training); dim(testing)
qplot(Petal.Width, Sepal.Width, color=Species, data=training)
qplot(Petal.Width, Sepal.Width, colour=Species, data=training)
modFit <- train(Species ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree"
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattel)
install.packages("rattel")
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
help(fancyRpartPlot)
modFit$finalModel
fancyRpartPlot(modFit$finalModel)
help(plot)
rattle()
fancyRpartPlot(modFit$finalModel)
predict(modFit, newdata=testing)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(ozone, package="ElemStatLearn")
head(ozone)
ss <- sample(1:dim(ozone)[1], replace=T)
head(ss)
ll <- matrix(NA, nrow=10, ncol=155)
for(i in 1:10) {
ss <- sample(1:dim(ozone)[1], replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone, data=ozone0, span=0.2)
ll[i,] <- predict(loess0, newdata.frame(ozone=1:155))
}
ll <- matrix(NA, nrow=10, ncol=155)
for(i in 1:10) {
ss <- sample(1:dim(ozone)[1], replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone, data=ozone0, span=0.2)
ll[i,] <- predict(loess0, newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
for(i in 1:10) {lines(1:155, ll[i,], col="grey", lwd=2)}
lines(1:155, apply(11,2,mean), col="red", lwd=2)
plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
for(i in 1:10) {lines(1:155, ll[i,], col="grey", lwd=2)}
lines(1:155, apply(ll,2,mean), col="red", lwd=2)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B=10, bagControl=bagControl(fit=ctreeBag$fit,
predict=ctreeBag$pred,
aggregate=ctreeBag$aggregate))
plot(ozone$ozone, ozone$temperature, col='lightgrey', pch=19)
plot(ozone$ozone, predict(treebag$fit[[1]]$fit,predictors), col='red', pch=19)
plot(ozone$ozone, predict(treebag,predictors), col='blue', pch=19)
treebag <- bag(predictors, temperature, B=10,
bagControl=bagControl(fit=ctreeBag$fit,
predict=ctreeBag$pred,
aggregate=ctreeBag$aggregate))
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(traning); dim(testing)
data(iris)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(traning); dim(testing)
dim(training); dim(testing)
modFit <- train(Species ~ ., data=training, method="rf", prox=TRUE)
modFit <- train(Species ~ ., data=training, method="rf", prox=TRUE)
modFit
getTree(modFit$finalModel, k=2)
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species, data=training)
p
p + geom_point(aes(x=Petal.Width, y=Petal.Length, col=Species), size=5, shape=4, data=irisP)
help(aes)
pred <- predict(modFit, testing); testing$predRight <- pred==testing$Species
table(pred, testing$Species)
gplot(Petal.Width, Petal.Length, col=predRight, data=testing, main="newdata Preditions")
qplot(Petal.Width, Petal.Length, col=predRight, data=testing, main="newdata Preditions")
clear
clr
library(ISLR)
install.packages("ISLR")
library(ISLR)
data(Wage)
name(Wage)
names(Wage)
Wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
print(modFit)
modFit <- train(Wage ~ ., method="gbm", data=training, verbose=FALSE)
print(modFit)
qplot(predict(modFit, testing), wage, data=testing)
data(iris)
library(ggplot2)
library(caret)
names(iris)
table(iris$Species)
modlda = train(Species ~ ., data=training, method="lda")
modnb = train(Species ~ ., data=training, method="nb")
plda = predict(modlda, testing); pnb = predict(modnb, testing)
table(plda, pnb)
modlda = train(Species ~ ., data=training, method="lda")
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
modlda = train(Species ~ ., data=training, method="lda")
modnb = train(Species ~ ., data=training, method="nb")
modlda = train(Species ~ ., data=training, method="lda")
modnb = train(Species ~ ., data=training, method="nb")
plda = predict(modlda, testing); pnb = predict(modnb, testing)
table(plda, pnb)
equalPredictions = (plda == pnb)
qplot(Petal.Width, Sepal.Width, color=equalPredictions, data=testing)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
dim(training); dim(testing)
modFit <- train(Case ~ ., method="rpart", data=training)
seed(125)
seeds(125)
set.seed(125)
modFit <- train(Case ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Case ~ ., method="rpart", data=training)
print(modFit$finalModel)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Case ~ ., method="rpart", data=training)
print(modFit$finalModel)
library(AppliedPredictiveModeling)
library(caret)
library(rattle)
rm(list = ls())
data(segmentationOriginal)
names(segmentationOriginal)
training = subset(segmentationOriginal, Case == "Train")
testing = subset(segmentationOriginal, Case == "Test")
dim(training); dim(testing)
set.seed(125)
modFit = train(Case ~ ., method="rpart", data=training)
print(modFit$finalModel)
set.seed(125)
model = train(Case ~ ., method="rpart", data=training)
print(model)
set.seed(125)
model = train(Case ~ ., method="rpart", data=training)
model = train(Case ~ ., method='rpart', data=training)
set.seed(125)
modFit <- train(Case ~ ., method="rpart", data=training)
training = subset(segmentationOriginal, Case == "Train")
testing = subset(segmentationOriginal, Case == "Test")
dim(training); dim(testing)
set.seed(125)
modFit <- tra
library(ElemStatLearn); data(prostate)
str(prostate)
dim(ptostate)
dim(prostate)
names(prostate)
small = prostate[1:5,]
lm(lpsa ~ ., data=small)
help(lm)
library(ISLR); data(Wage)
library(ggplot2); library(caret)
str(Wage)
Wage <- subset(Wage, select=-c(logwage))
str(Wage)
inBuild <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage, p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]
dim(training); dim(testing); dim(validation)
mod1 <- train(wage ~., method="glm", data=training)
## Random Forests with 3 fold cross validation
mod2 <- train(wage ~., method="rf", data=traing, trControl=trainControl(method="cv"),number=3)
mod2 <- train(wage ~., method="rf", data=training, trControl=trainControl(method="cv"),number=3)
pred1 <- predict(mod1, testing); pred2 <- predict(mod2, testing)
table(pred1, pred2)
qplot(pred1, pred2, color=wage, data=testing)
confusionMatrix(pred1, pred2)
pred1
pred2
sqrt(sum((pred1-testing$wagr)^2))
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
predDF <- data.frame(pred1, pred2, wage=testing$wage)
combModFit <- train(wage ~., method="gam", data=predDF)
combPred <- predict(combModFit, predDF)
combPred
sqrt(sum((combPred-testing$wage)^2))
combPred <- predict(combModFit, testing)
sqrt(sum((combPred-testing$wage)^2))
pred1V <- predict(pred1, validation)
pred2V <- predict(pred2, validation)
predVDF <- data.frame(pred1=pred1V, pred2=pred2V)
combPredV <- predict(combModFit, predVDF)
pred1V <- predict(mod1, validation)
pred2V <- predict(mod2, validation)
predVDF <- data.frame(pred1=pred1V, pred2=pred2V)
combPredV <- predict(combModFit, predVDF)
sqrt(sum((pred1V-validation$wage)^2)); sqrt(sum((pred2V-validation$wage)^2)); sqrt(sum((combPredV-validation$wage)^2))
help(getSymbols)
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m%d%y")
to.dat <- as.Date("12/31/13", format="%m%d%y")
install.packages("quantmod")
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m%d%y")
to.dat <- as.Date("12/31/13", format="%m%d%y")
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
from.dat
library(quantmod)
help(getSymbols)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
from.dat
to.dat
mGoog <- to.monthly(GOOG)
googOpen <- Op(mGoog)
tsl <- ts(googOpen, frequency=12)
plot(ts1, xlab="Years+1", ylab="GOOG")
mGoog <- to.monthly(GOOG)
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
head(GOOG)
data(iris)
library(ggplot)
data(iris)
library(ggplot2)
# Iris example ignoring species labels
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training), dim(testing)
library(ggplot2)
# Iris example ignoring species labels
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training), dim(testing)
dim(training); dim(testing)
kMeans1 <- kmeans(subset(training, select=-c(Species)), centers=3)
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width, Petal.Length, color=clusters, data=training)
table(kMeans1$cluster, training$Species)
modFit <- train(clusters ~., data=subset(training,select=-c(Species)), method="rpart")
table(predict(modFit, training), training$Species)
testClusterPred <- predict(modFit, testing)
table(testClusterPred, testing$Species)
help(factor)
library(ElemStatLearn)
library(caret)
data(vowel.train)
data(vowel.test)
rm(list = ls())
# Set the variable y to be a factor variable in both the training and test set.
vowel.train$y = factor(vowel.train$y)
vowel.test$y = factor(vowel.test$y)
dim(vowel.train)
set.seed(33833)
mod1 <- train(y ~., method="rf", data=vowel.train)
mod2 <- train(y ~., method="gbm", data=vowel.train)
mod1
mod2
pred1 <- predict(mod1, vowel.test); pred2 <- predict(mod2, vowel.test)
pred1; pred2
confusionMatrix(pred1, pred2)
confusionMatrix(pred1, vowel.test$y)
confusionMatrix(pred2, vowel.test$y)
help(confusionMatrix)
pred1 == pred2
(pred1 == pred2) && (pred2 == vowel.test$y)
(pred1 == pred2) & (pred2 == vowel.test$y)
(pred1 == pred2) & (pred2 == vowel.test$y)
sum((pred1 == pred2) & (pred2 == vowel.test$y)) / length(pred1)
sum((pred1 == vowel.test$y)) / length(vowel.test$y)
sum((pred2 == vowel.test$y)) / length(vowel.test$y)
sum((pred1 == pred2) & (pred2 == vowel.test$y)) / length(vowel.test$y)
sum((pred2 == pred2)) / length(vowel.test$y)
sum((pred1 == pred2)) / length(vowel.test$y)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
mod_rf <- train(diagnosis ~., method="rf", data=training)
mod_gbm <- train(diagnosis ~., method="gbm", data=training)
mod_lda <- train(diagnosis ~., method="lda", data=training)
pred_rf <- predict(mod_rf, training); pred_gbm <- predict(mod_gbm, training); pred_lda <- predict(mod_lda, training)
predDF <- data.frame(rf=pred_rf, gbm=pred_gbm, lda=pred_lda, diagnosis=training$diagnosis)
mod_comb <- train(diagnosis ~., method="rf", data=predDF)
pred_rf_test <- predict(mod_rf, testing)
pred_gbm_test <- predict(mod_gbm, testing)
pred_lda_test <- predict(mod_lda, testing)
comb_data_test <- data.frame(rf=pred_rf_test, gbm=pred_gbm_test, lda=pred_lda_test, diagnosis=testing$diagnosis)
pred_comb_test <- predict(mod_comb, testing)
pred_comb_test <- predict(mod_comb, comb_data_test)
accuracy_rf = sum(pred_rf_test == testing$diagnosis) / length(testing$diagnosis)
accuracy_gbm = sum(pred_gbm_test == testing$diagnosis) / length(testing$diagnosis)
accuracy_lda = sum(pred_lda_test == testing$diagnosis) / length(testing$diagnosis)
accuracy_comb = sum(pred_comb_test == comb_data_test$diagnosis) / length(testing$diagnosis)
accuracy_rf; accuracy_gbm; accuracy_lda; accuracy_comb
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
model = train(CompressiveStrength ~ ., method = 'lasso', data = training)
model
plot(model$finalModel)
plot.enet(model_q3$finalModel, xvar="penalty",use.color = TRUE)
plot.enet(model$finalModel, xvar="penalty",use.color = TRUE)
install.package("forecast")
library(forecast)
install.package("forecast")
install.packages("forecast")
library(forecast)
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages("lubridate")
library(forecast)
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
library(lubridate)
install.packages("forecast")
library(forecast)
setwd("D:/4 - Works/GitHub/MyStudy/R_code")
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
help(bats)
fit <- bats(training)
plot(forecast(fit))
head(training)
fit <- bats(tstrain)
plot(forecast(fit))
help(nrow)
pred <- forecast(fit, level=95, nrow(testing))
plot(pred)
sum(testing$visitsTumblr > pred$lower & testing$visitsTumblr < pred$upper) / nrow(testing)
accuracy(pred, testing$visitsTumblr)
library(AppliedPredictiveModeling)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library(e1071)
install.packages("e1071")
install.packages("e1071")
library(e1071)
set.seed(352)
library(e1071)
mod <- svm(CompressiveStrength ~., data=training)
pred <- predict(mod, testing)
accuracy(pred, testing$CompressiveStrength)
library(caret)
accuracy(pred, testing$CompressiveStrength)
library(ggplot2)
accuracy(pred, testing$CompressiveStrength)
help(accuracy)
install.packages("forecast")
library(forecast)
help(accuracy)
accuracy(pred, testing$CompressiveStrength)
RMSE = sqrt(sum((pred - testing$CompressiveStrength)^2))
RMSE
predins = predict(mod, training)
RMSEins = sqrt(sum((predins - training$CompressiveStrength)^2))
RMSEins
help(RMSE)
help(accuracy)
help(mean)
RMSE = sqrt(mean((pred - testing$CompressiveStrength)^2))
RMSE
